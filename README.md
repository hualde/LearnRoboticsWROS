# ü§ñ Proyecto Pick and Place con Visi√≥n - UR5 + Robotiq Gripper

Sistema completo de pick and place guiado por visi√≥n para robot UR5 con gripper Robotiq en ROS 2 Humble.

---

## üìã Tabla de Contenidos

- [Descripci√≥n General](#descripci√≥n-general)
- [Arquitectura del Sistema](#arquitectura-del-sistema)
- [Requisitos](#requisitos)
- [Instalaci√≥n](#instalaci√≥n)
- [Uso](#uso)
- [Componentes](#componentes)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Troubleshooting](#troubleshooting)
- [Desarrollo Futuro](#desarrollo-futuro)

---

## üéØ Descripci√≥n General

Este proyecto implementa un sistema completo de manipulaci√≥n rob√≥tica que integra:

- ‚úÖ **Detecci√≥n visual de objetos** por color (RGB-D camera)
- ‚úÖ **Transformaci√≥n de coordenadas** (p√≠xel ‚Üí 3D ‚Üí frame del robot)
- ‚úÖ **Planificaci√≥n de movimientos** con MoveIt 2
- ‚úÖ **Control del brazo rob√≥tico** UR5 (6 DOF)
- ‚úÖ **Control del gripper** Robotiq 85
- ‚úÖ **Simulaci√≥n completa** en Gazebo

### Flujo del Sistema

```
üì∑ C√°mara RGB-D
    ‚Üì
üîç Detecci√≥n de objetos (OpenCV)
    ‚Üì
üìç Coordenadas 3D (PointCloud ‚Üí TF)
    ‚Üì
üì° Publicaci√≥n en /detected_objects
    ‚Üì
üß† Planificaci√≥n con MoveIt
    ‚Üì
ü§ñ Ejecuci√≥n en UR5
    ‚Üì
‚úã Control del gripper
```

---

## üèóÔ∏è Arquitectura del Sistema

### Hardware (Simulado)
- **Robot:** Universal Robots UR5 (6 DOF)
- **Gripper:** Robotiq 85
- **Sensor:** C√°mara RGB-D (tipo Kinect)
- **Entorno:** Gazebo con world2 personalizado

### Software Stack
- **ROS 2:** Humble
- **Simulaci√≥n:** Gazebo Classic
- **Planificaci√≥n:** MoveIt 2
- **Visi√≥n:** OpenCV + sensor_msgs
- **Control:** ros2_control

### Nodos Principales

| Nodo | Funci√≥n | Topics Pub/Sub |
|------|---------|----------------|
| `vision_detector` | Detecta objetos y calcula posiciones 3D | Pub: `/detected_objects`, `/object_markers` |
| `robot_mover_action` | Control del brazo con MoveIt | Sub: `/joint_states`, Action: `/move_action` |
| `gripper_controller` | Control del gripper Robotiq | Action: `/gripper_position_controller/gripper_cmd` |
| `pick_place_main` | Coordinador del pipeline completo | Sub: `/detected_objects` |

---

## üì¶ Requisitos

### Sistema Operativo
- Ubuntu 22.04 LTS

### ROS 2
```bash
# ROS 2 Humble
sudo apt install ros-humble-desktop-full
sudo apt install ros-humble-moveit
sudo apt install ros-humble-ros2-control
sudo apt install ros-humble-ros2-controllers
sudo apt install ros-humble-gazebo-ros-pkgs
sudo apt install ros-humble-moveit-ros-perception
```

### Dependencias Python
```bash
# OpenCV y CV Bridge
sudo apt install ros-humble-cv-bridge python3-opencv

# Scipy (para transformaciones)
pip3 install scipy

# Sensor messages
sudo apt install ros-humble-sensor-msgs-py
```

### Paquetes del Proyecto
- `ur_yt_sim` - Simulaci√≥n y configuraci√≥n del robot
- `ur5_camera_gripper_moveit_config` - Configuraci√≥n de MoveIt
- `ur5_pick_place` - Scripts de control y visi√≥n

---

## üöÄ Instalaci√≥n

### 1. Clonar el Repositorio

```bash
cd ~/misCosas/ros2_ws/src
git clone <URL_DEL_REPO> LearnRoboticsWROS
```

### 2. Instalar Dependencias

```bash
cd ~/misCosas/ros2_ws
rosdep install --from-paths src --ignore-src -r -y
```

### 3. Compilar

```bash
cd ~/misCosas/ros2_ws
colcon build
source install/setup.bash
```

### 4. Verificar Instalaci√≥n

```bash
# Listar paquetes instalados
ros2 pkg list | grep ur

# Deber√≠a mostrar:
# - ur_yt_sim
# - ur5_camera_gripper_moveit_config
# - ur5_pick_place
```

---

## üéÆ Uso

### Lanzar el Sistema Completo

#### Terminal 1: Simulaci√≥n + MoveIt
```bash
cd ~/misCosas/ros2_ws
source install/setup.bash
ros2 launch ur_yt_sim spawn_ur5_camera_gripper_moveit.launch.py
```

**Esto inicia:**
- ‚úÖ Gazebo con world2
- ‚úÖ Robot UR5 con gripper
- ‚úÖ C√°mara RGB-D
- ‚úÖ MoveIt con RViz
- ‚úÖ Controladores (brazo + gripper)

#### Terminal 2: Sistema de Visi√≥n
```bash
cd ~/misCosas/ros2_ws/src/LearnRoboticsWROS/ur_yt_sim/scripts
python3 vision_detector.py
```

**Esto publica:**
- `/detected_objects` - JSON con objetos y posiciones 3D
- `/object_poses` - PoseArray para MoveIt
- `/object_markers` - Marcadores para RViz

#### Terminal 3: Control del Robot
```bash
cd ~/misCosas/ros2_ws/src/LearnRoboticsWROS/ur5_pick_place/scripts/API_moveit
python3 robot_mover_action.py
```

**Men√∫ interactivo:**
```
1. Mover a HOME
2. Mover a ZERO
3. Mover a UP
```

---

## üß© Componentes

### 1. Sistema de Visi√≥n (`vision_detector.py`)

**Funcionalidad:**
- Detecta objetos de colores: rojo, azul, verde, amarillo, naranja
- Obtiene coordenadas 3D desde la imagen de profundidad
- Transforma coordenadas de `camera_link` a `base_link`
- Publica resultados en formato JSON y marcadores 3D

**Configuraci√≥n:**
```python
# Rangos de color HSV (modificables)
'red': [(0, 100, 100), (10, 255, 255)]
'green': [(40, 100, 100), (80, 255, 255)]
'blue': [(100, 100, 100), (130, 255, 255)]
```

**Topics:**
- `/detected_objects` (String/JSON)
- `/object_poses` (PoseArray)
- `/object_markers` (MarkerArray)

**Ejemplo de salida:**
```json
[
  {
    "id": 0,
    "color": "green",
    "pixel_x": 320,
    "pixel_y": 240,
    "area": 1234.5,
    "position_camera_frame": {"x": 0.456, "y": 0.123, "z": 0.852},
    "position_robot_frame": {"x": 0.523, "y": -0.034, "z": 1.652}
  }
]
```

---

### 2. Control del Robot (`robot_mover_action.py`)

**Funcionalidad:**
- Usa Move Action API (mismo que RViz)
- Mueve a poses predefinidas (home, zero, up)
- Planificaci√≥n y ejecuci√≥n con MoveIt
- Control preciso de velocidad y aceleraci√≥n

**Poses Predefinidas:**
```python
"home": [0.0, -2.2564, 1.4059, -1.6315, -1.57, 0.0]
"zero": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
"up": [0.0, -1.57, 0.0, -1.57, 0.0, 0.0]
```

**Par√°metros de Movimiento:**
- `max_velocity_scaling_factor`: 0.1 (10% velocidad m√°xima)
- `max_acceleration_scaling_factor`: 0.1 (10% aceleraci√≥n m√°xima)
- `allowed_planning_time`: 5.0 segundos
- `num_planning_attempts`: 10

---

### 3. C√°mara RGB-D

**Especificaciones:**
- **Tipo:** Depth camera (RGB + profundidad)
- **Resoluci√≥n:** 640x480 p√≠xeles
- **Frecuencia:** 10 Hz
- **Campo de visi√≥n:** 62.4¬∞ horizontal
- **Rango:** 5cm - 8m
- **Posici√≥n:** Fija a 1.2m altura, mirando hacia abajo

**Topics:**
- `/camera/image_raw` - Imagen RGB
- `/camera/depth/image_raw` - Mapa de profundidad
- `/camera/points` - Nube de puntos 3D
- `/camera/camera_info` - Par√°metros intr√≠nsecos

**Modificar posici√≥n:**
```bash
nano ~/misCosas/ros2_ws/src/LearnRoboticsWROS/ur_yt_sim/urdf/camera.xacro
# L√≠nea 36: <origin xyz="0.5 0 1.2" rpy="0 1.57 0"/>
```

---

### 4. Control del Gripper

**Especificaciones:**
- **Modelo:** Robotiq 85
- **Apertura:** 0 - 85mm
- **Joint principal:** `robotiq_85_left_knuckle_joint`
- **Posiciones:** Abierto (0.0) / Cerrado (0.79)

**Action:**
```bash
/gripper_position_controller/gripper_cmd
```

**Uso desde Python:**
```python
from control_msgs.action import GripperCommand

goal = GripperCommand.Goal()
goal.command.position = 0.0  # Abierto
goal.command.max_effort = 100.0
```

---

### 5. Secuencia de Aprendizaje (`learning_moveit/`)

Para fines educativos y pruebas industriales, se ha incluido una serie de scripts secuenciales en `ur5_pick_place/scripts/learning_moveit/`.

#### **Script Principal: `06_sequential_pick_place.py`**
Este script realiza una secuencia completa de Pick & Place con las siguientes especificaciones actuales:

**Punto de Recogida (PICK):**
- **Posici√≥n (XYZ):** `[0.500, 0.000, 0.379]`
- **Orientaci√≥n (Quaternion xyzw):** `[0.737, -0.675, 0.020, 0.006]`

**Punto de Descarga (PLACE):**
- **Posici√≥n (XYZ):** `[0.400, 0.545, 0.515]`
- **Orientaci√≥n (Quaternion xyzw):** `[0.665, -0.600, 0.310, -0.320]`

---

## üìÅ Estructura del Proyecto

```
LearnRoboticsWROS/
‚îú‚îÄ‚îÄ ur_yt_sim/                           # Paquete principal de simulaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ launch/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spawn_ur5_camera_gripper_moveit.launch.py
‚îÇ   ‚îú‚îÄ‚îÄ urdf/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ur.urdf.xacro                # URDF principal
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ camera.xacro                 # Configuraci√≥n de c√°mara
‚îÇ   ‚îú‚îÄ‚îÄ worlds/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ world2.world                 # Mundo con objetos
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ur5_controllers.yaml         # Configuraci√≥n controladores
‚îÇ   ‚îî‚îÄ‚îÄ scripts/
‚îÇ       ‚îî‚îÄ‚îÄ vision_detector.py           # Sistema de visi√≥n
‚îÇ
‚îú‚îÄ‚îÄ ur5_camera_gripper_moveit_config/    # Configuraci√≥n MoveIt
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ur.srdf                      # Sem√°ntica del robot
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ moveit_controllers.yaml     # Controladores MoveIt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kinematics.yaml              # Cinem√°tica (IK/FK)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ moveit.rviz                  # Configuraci√≥n RViz
‚îÇ   ‚îî‚îÄ‚îÄ launch/
‚îÇ       ‚îî‚îÄ‚îÄ moveit.launch.py
‚îÇ
‚îî‚îÄ‚îÄ ur5_pick_place/                      # Scripts de control
    ‚îî‚îÄ‚îÄ scripts/
        ‚îî‚îÄ‚îÄ API_moveit/
            ‚îú‚îÄ‚îÄ robot_mover_action.py    # Control del robot
            ‚îî‚îÄ‚îÄ gripper_controller.py    # Control del gripper (futuro)
```

---

## üîß Configuraci√≥n

### Archivos Clave

#### 1. SRDF - Configuraci√≥n Sem√°ntica
**Ubicaci√≥n:** `ur5_camera_gripper_moveit_config/config/ur.srdf`

**Planning Groups:**
- `ur5_manipulator` - Brazo (6 DOF)
- `robotiq_gripper` - Gripper

**Poses Predefinidas:**
- `home`, `zero`, `up` (brazo)
- `open`, `close` (gripper)

#### 2. Controllers - Configuraci√≥n de Control
**Ubicaci√≥n:** `ur5_camera_gripper_moveit_config/config/moveit_controllers.yaml`

```yaml
joint_trajectory_controller:
  type: FollowJointTrajectory
  joints:
    - shoulder_pan_joint
    - shoulder_lift_joint
    - elbow_joint
    - wrist_1_joint
    - wrist_2_joint
    - wrist_3_joint

gripper_position_controller:
  type: GripperCommand
  joints:
    - robotiq_85_left_knuckle_joint
```

#### 3. World - Entorno de Simulaci√≥n
**Ubicaci√≥n:** `ur_yt_sim/worlds/world2.world`

**Contiene:**
- Mesa de trabajo (1.2m x 1.2m)
- Objetos de colores para agarrar
- Iluminaci√≥n
- F√≠sica configurada

---

## üé® Visualizaci√≥n en RViz

### Agregar Marcadores de Objetos

1. En RViz, clic en **"Add"**
2. Seleccionar **"MarkerArray"**
3. Topic: `/object_markers`
4. ‚úÖ Ver√°s esferas de colores en las posiciones de los objetos

### Ver Imagen de C√°mara

1. Clic en **"Add"**
2. Seleccionar **"Image"**
3. Topic: `/camera/image_raw`

### Ver Nube de Puntos

1. Clic en **"Add"**
2. Seleccionar **"PointCloud2"**
3. Topic: `/camera/points`
4. Color Transformer: **RGB8**

---

## üß™ Testing

### Verificar C√°mara

```bash
# Ver topics
ros2 topic list | grep camera

# Ver frecuencia
ros2 topic hz /camera/image_raw

# Ver imagen
ros2 run rqt_image_view rqt_image_view
```

### Verificar Controladores

```bash
# Listar controladores
ros2 control list_controllers

# Ver joint states
ros2 topic echo /joint_states --once
```

### Verificar MoveIt

```bash
# Ver si move_group est√° activo
ros2 node list | grep move_group

# Ver action servers
ros2 action list
```

### Verificar Transformaciones

```bash
# Ver √°rbol TF
ros2 run tf2_tools view_frames
evince frames.pdf

# Ver transformaci√≥n espec√≠fica
ros2 run tf2_ros tf2_echo base_link camera_link
```

---

## üêõ Troubleshooting

### Problema: "Planning failed! Error code: FAILURE"

**Causa:** MoveIt no puede encontrar una trayectoria v√°lida

**Soluciones:**
1. Verificar que el objetivo est√° dentro del workspace
2. Probar con `num_planning_attempts` mayor
3. Aumentar `allowed_planning_time`
4. Verificar colisiones en RViz

```python
goal_msg.request.num_planning_attempts = 20
goal_msg.request.allowed_planning_time = 10.0
```

---

### Problema: C√°mara no detecta objetos

**Verificaciones:**
```bash
# 1. ¬øLa c√°mara publica?
ros2 topic hz /camera/image_raw

# 2. ¬øHay objetos en la escena?
# Verificar en Gazebo

# 3. ¬øLos rangos de color son correctos?
# Ajustar HSV en vision_detector.py
```

**Ajustar rangos HSV:**
```python
# vision_detector.py, l√≠nea ~65
'green': [(40, 100, 100), (80, 255, 255)]  # Ampliar rango si es necesario
```

---

### Problema: Robot no se mueve desde Python

**Debug:**
```bash
# 1. ¬øFunciona desde RViz?
# Probar "Plan & Execute" en RViz

# 2. ¬øLos controladores est√°n activos?
ros2 control list_controllers

# 3. ¬øEl action server responde?
ros2 action send_goal /move_action moveit_msgs/action/MoveGroup "{}"
```

---

### Problema: Transformaciones TF incorrectas

**Verificar:**
```bash
# Ver frames disponibles
ros2 run tf2_tools view_frames

# Ver transformaci√≥n
ros2 run tf2_ros tf2_echo base_link camera_link

# Debe mostrar:
# Translation: [0.500, 0.000, 1.200]
```

**Corregir en camera.xacro:**
```xml
<origin xyz="0.5 0 1.2" rpy="0 1.57 0"/>
```

---

### Problema: Gazebo se congela

**Causas comunes:**
- F√≠sica muy pesada
- Demasiados objetos
- GPU insuficiente

**Soluciones:**
```bash
# 1. Reducir calidad gr√°fica
export LIBGL_ALWAYS_SOFTWARE=1

# 2. Pausar Gazebo cuando no se usa
# Clic en "Pause" en Gazebo GUI

# 3. Reducir real_time_update_rate en world
# En world2.world:
<real_time_update_rate>500.0</real_time_update_rate>
```

---

## üìä Par√°metros Ajustables

### Velocidad del Robot

```python
# En robot_mover_action.py
goal_msg.request.max_velocity_scaling_factor = 0.1  # 0.1 = 10%
goal_msg.request.max_acceleration_scaling_factor = 0.1
```

### Detecci√≥n de Colores

```python
# En vision_detector.py
color_ranges = {
    'green': [(40, 100, 100), (80, 255, 255)]  # [H_min, S_min, V_min], [H_max, S_max, V_max]
}
```

### Frecuencia de Detecci√≥n

```python
# En vision_detector.py, l√≠nea ~45
self.process_rate = 2.0  # Hz (detecciones por segundo)
```

### C√°mara

```xml
<!-- En camera.xacro -->
<update_rate>10</update_rate>  <!-- FPS -->
<horizontal_fov>1.089</horizontal_fov>  <!-- Campo de visi√≥n en radianes -->
<image><width>640</width><height>480</height></image>  <!-- Resoluci√≥n -->
```

---

## üöÄ Desarrollo Futuro

### Corto Plazo (Pr√≥ximos pasos)
- [ ] Implementar secuencia completa de pick
- [ ] Implementar secuencia completa de place
- [ ] Pipeline autom√°tico pick and place
- [ ] Control del gripper integrado

### Mediano Plazo
- [ ] Detecci√≥n con deep learning (YOLO)
- [ ] Estimaci√≥n de pose 6D de objetos
- [ ] Planificaci√≥n de trayectorias optimizada
- [ ] Manejo robusto de errores

### Largo Plazo
- [ ] M√∫ltiples objetos en paralelo
- [ ] Clasificaci√≥n y sorting por color
- [ ] Apilamiento de objetos
- [ ] Aprendizaje por refuerzo

---

## üìö Referencias

### Documentaci√≥n Oficial
- [ROS 2 Humble](https://docs.ros.org/en/humble/)
- [MoveIt 2](https://moveit.picknik.ai/main/index.html)
- [Gazebo Classic](https://classic.gazebosim.org/)
- [ros2_control](https://control.ros.org/master/index.html)

### Tutoriales Relacionados
- [MoveIt Python API](https://moveit.picknik.ai/main/doc/examples/motion_planning_python_api/motion_planning_python_api_tutorial.html)
- [OpenCV Python](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)
- [TF2 Tutorials](https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Tf2-Main.html)

---

## üë• Contribuciones

Sugerencias para contribuir:
1. Fork del repositorio
2. Crear branch (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push al branch (`git push origin feature/nueva-funcionalidad`)
5. Abrir Pull Request

---

## üìù Licencia

Este proyecto es de c√≥digo abierto bajo licencia [especificar licencia].

---

## üôè Agradecimientos

- Universal Robots por documentaci√≥n del UR5
- Robotiq por especificaciones del gripper
- Comunidad de MoveIt
- Comunidad de ROS 2

---

## üìß Contacto

Para preguntas o soporte:
- **Email:** [tu email]
- **GitHub:** [tu github]
- **ROS Discourse:** [tu usuario]

---

## üéì Cr√©ditos

Proyecto desarrollado como parte de aprendizaje en rob√≥tica y visi√≥n artificial con ROS 2.

**Tecnolog√≠as utilizadas:**
- ROS 2 Humble
- MoveIt 2
- Gazebo Classic
- OpenCV
- Python 3.10
- C++ (URDF/xacro)

---

**√öltima actualizaci√≥n:** Enero 2026  
**Versi√≥n:** 1.0.0  
**Estado:** En desarrollo activo

---
#Para no visualizar errores en consola
```bash
ros2 launch ur_yt_sim spawn_ur5_camera_gripper_moveit.launch.py 2>&1 | grep -v "Joint.*mimic.*not found"
```
#Sin octomap
```bash
ros2 launch ur_yt_sim spawn_ur5_camera_gripper_moveit.launch.py with_octomap:=false 2>&1 | grep -v "Joint.*mimic.*not found"
```
#A√±adimos colisiones
```bash
ros2 run ur_yt_sim test_ik_collision_obj
```

#Servicio pinza
```bash
ros2 service call /ATTACHLINK linkattacher_msgs/srv/AttachLink "{
  model1_name: 'cobot',
  link1_name: 'wrist_3_link',
  model2_name: 'cube_pick',
  link2_name: 'link_1'
}"
ros2 service call /DETACHLINK linkattacher_msgs/srv/DetachLink "{
  model1_name: 'cobot',
  link1_name: 'wrist_3_link',
  model2_name: 'cube_pick',
  link2_name: 'link_1'
}"

```

## üê≥ Docker

### Quick Start
```bash
# Pull from Docker Hub
docker pull hualde/ur5-robotiq85:latest

# Allow X11 forwarding
xhost +si:localuser:root

# Run container
docker run -it --rm \
  --network host \
  -e DISPLAY=$DISPLAY \
  -e QT_X11_NO_MITSHM=1 \
  -v /tmp/.X11-unix:/tmp/.X11-unix:rw \
  -v /dev/dri:/dev/dri \
  hualde/ur5-robotiq85:latest

# Inside container, launch simulation
ros2 launch ur_yt_sim spawn_ur5_camera_gripper_moveit.launch.py 2>&1 | grep -v "Joint.*mimic.*not found"

```

### Building from Source
```bash
git clone https://github.com/YOUR_USERNAME/LearnRoboticsWROS.git
cd LearnRoboticsWROS
docker build -t ur5-robotiq85:latest .
```

### Docker Hub
Image available at: https://hub.docker.com/r/hualde/ur5-robotiq85

### What's Included
- ROS2 Humble Desktop Full
- Gazebo simulation
- MoveIt2 motion planning
- UR5 robot with Robotiq 85 gripper
- Camera integration
- Pick and place capabilities

### System Requirements
- Ubuntu 22.04 (or WSL2 on Windows)
- Docker installed
- X11 for GUI (Gazebo/RViz)
- GPU drivers for hardware acceleration
```

Guardar: `Ctrl+O`, `Enter`, `Ctrl+X`

---

## Cuando el build termine exitosamente

Ver√°s algo como:
```
 => exporting to image
 => => exporting layers
 => => writing image sha256:abc123...
 => => naming to docker.io/library/ur5-robotiq85:latest
Successfully built abc123def456
Successfully tagged ur5-robotiq85:latest